<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>探索发现好玩的东西</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 15 Sep 2018 10:36:42 +0800</pubDate>
    <lastBuildDate>Sat, 15 Sep 2018 10:36:42 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>从Git规范谈软件开发流程</title>
        <description>
&lt;p&gt;一般软件产品从需求到正式上线分成这几个过程，需要RD、OPS、QA共同参与，如果有工具将三者有机的结合起来，提高开发效率，就是我们常说的DevOps。
一般通过平台实现DevOps的半自动化，包含 需求，设计，编码，测试，部署，监控，反馈，然后迭代循环。
快速迭代的内部产品，往往要求程序员具备从需求分析，编码自测到快速部署的能力，能够快速验证并迭代产品，据说这是Amazon要求程序员做的基本功。
下面就几个关键步骤说一下我自己的理解，包括需求，编码，测试，部署这几个模块。&lt;/p&gt;

&lt;h2 id=&quot;日常需求管理&quot;&gt;日常需求管理&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;所有的需求录入到devops需求平台，指定责任人，时间周期&lt;/li&gt;
  &lt;li&gt;自测完毕，意味着开发结束，转发指派给相关的使用方&lt;/li&gt;
  &lt;li&gt;需求可以由组长统一处理并指派.&lt;/li&gt;
  &lt;li&gt;需求尽量拆分为一些小的可跟踪，可排期的需求，一般不超过一周。&lt;/li&gt;
  &lt;li&gt;任务和需求的区别：任务是不需要测试和上线的，可以理解成一些日常的零碎活儿。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;git开发规范和质量控制&quot;&gt;GIT开发规范和质量控制&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Git-flow规范
规范的多人协作有助于减少bug和系统风险，比如出现线上问题导致reset回滚。原则就是按照套路来，feature、release以及hotfix分支的正确合并让dev和master分支能有条不紊的迭代。对于小团队来说，原则上可以经过周知，每人自行合并review过feature分支到develop，但不可以合并到master分支上。master上必须是经过测试后的devleop代码。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;常用的Git-flow分支模型规范
&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/1226129-b2018af358d865d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700&quot; alt=&quot;Git-flow&quot; /&gt;&lt;/p&gt;
    &lt;h3 id=&quot;简要说明&quot;&gt;简要说明&lt;/h3&gt;
  &lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;
develop分支是比较激进的开发分支，对应本地最新的从feature分支合并过来的开发代码。
master分支是稳定的分支，对应生产环境代码，merge到master的时候请加上版本标签(git tag)，相当于线上环境的备份。
feature分支是功能分支，从develop中来，向develop进行merge后删除feature分支
release分支是提测分支，从develop中来，测试有问题的话，直接修改后，向develop和master同步，没有问题，则不用修改。同步后删除release分支
hotfix分支是紧急修复分支，从master中来，向develop和master同步,然后删除hotfix
请不要直接修改develop和master分支，大的功能点拉出自己的feature-XX分支进行操作，小的功能可以拉topic-XX分支
常用操作
合并分支feature_1合并到develop
git checkout develop;
git pull --rebase (有冲突，手动解决之）
git merge --no-ff feature_1;(有冲突，手动解决之）
git tag -a V0.0.2;
git push --tags
git branch -d feature_1
git push origin :feature_1 删除远程分支，如果有的话
&lt;/pre&gt;
&lt;p&gt;一系列常规操作命令参考&lt;a href=&quot;https://www.cnblogs.com/cnblogsfans/p/5075073.html&quot;&gt;文章&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Git小知识点&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Git 模型
&lt;img src=&quot;http://www.ruanyifeng.com/blogimg/asset/2014/bg2014061202.jpg&quot; alt=&quot;模型&quot; /&gt;
workspace可以理解成工作区，提交之后才到仓库，也就是GIT的指针树中。
&lt;a href=&quot;http://www.ruanyifeng.com/blog/2014/06/git_remote.html&quot;&gt;来源&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Git分支设计理念&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;
可以想象GIT是一颗带有指针的树，节点是每次提交的Commit。树中有唯一的HEAD指针，同时有多个分支名指针。那么理解了这点，一个创建一个分支,只是在HEAD的指针位置添加一个分支指针而已。
推荐使用 merge --no-ff，因为合并能够清晰的保留历史。
rebase vs merge
共同点：都是做分支的commits合并
区别点：rebase会讲当前分支的改动作为path提交到目标分支的末尾，所以会变成一个序列，merge不会。
操作要点：“在自己分支使用merge --no-ff 或者rebase 主分支，主分支使用merge --no-ff,以保存所有合并历史。”
rebase一般在自己的分支上同步主分支使用，比如git pull , 然后使用git rebase master，不然自己埋头苦干，后来才发现合并到主分支一堆冲突。
参考
https://www.zhihu.com/question/36509119/answer/67828312
http://www.jianshu.com/p/c17472d704a0
rebase在公共分支上慎用，可能会导致公共分支变基，带来维护成本，推荐使用merge --no-ff模式，方便看出分支的来源，而不只为了看分支干净。
图片来源 [Git sandbox]( https://learngitbranching.js.org/?nodemo)
&lt;/pre&gt;
&lt;ul&gt;
  &lt;li&gt;git-flow所有分支历史的查看工具sourcetree&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;codereview&quot;&gt;codereview&lt;/h2&gt;

&lt;p&gt;一般使用gitlab配合phrabircator即可自动完成，在代码提交的时候进行强制codereview，有兴趣的同学可以查看facebook用的AK，也就是Phabricator.&lt;/p&gt;

&lt;h2 id=&quot;部署&quot;&gt;部署&lt;/h2&gt;
&lt;p&gt;不同的工程有不同的工具，这里就不统一介绍，但一般都包含线下环境和线上环境，支持一键部署。&lt;/p&gt;

</description>
        <pubDate>Mon, 12 Feb 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/2018/02/12/git-flow.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/2018/02/12/git-flow.html</guid>
        
        <category>Git</category>
        
        
        <category>工程开发</category>
        
      </item>
    
      <item>
        <title>从似然MLE推导最小二乘OLS谈机器学习三要素</title>
        <description>&lt;p&gt;最近被人问到最大似然，最小二乘，梯度下降，我发现一不留神很容易搞不清楚概念：对于机器学习三要素的问题，定义目标和怎么用算法达到这个目标的事儿，这几个概念清楚了，就知道梯度下降和前两者是不同的。下午写篇博客来梳理记录一下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;博客很久没有更新了，这段时间一直在琢磨工程项目上的事情，虽琐碎但比较充实。团队有关产研测的规范流转，使用devops类似的工具（具体内部工具就不说了）来去管理需求、开发、测试、上线部署、监控流程，实现进度和代码review等质量一系列的监管。开源的大家可以参考 &lt;strong&gt;Gitlab + Phabircator&lt;/strong&gt;. 开发规范可以参考&lt;strong&gt;Git-flow&lt;/strong&gt;。这不是本篇文章的重点，以后有时间可以详细说，&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;机器学习三要素&quot;&gt;机器学习三要素&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;模型：这里我们可以定义为非条件概率模型和条件概率模型，说的通俗一些就是常说的在从输入空间到输出空间的转化方式。&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;非条件概率&lt;/strong&gt;： &lt;script type=&quot;math/tex&quot;&gt;F=\{f\mid Y=f_\theta(X)\}&lt;/script&gt; 对应我们的判别式模型，比如LR SVM RNN CRF MEMM等，由于这种模型的偏差较小，方差较大，适用于数据量大的情况&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;条件概率&lt;/strong&gt;：&lt;script type=&quot;math/tex&quot;&gt;F = \{ P\mid P_\theta(Y\mid X),\theta \in R^n\}&lt;/script&gt; 对应我们的产生式模型，比如 NB HMM等，这种模型偏差大，方差小适用于数据量较小的情况。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;怎么选择模型&lt;/strong&gt;：有人问怎么调和这些方差偏差，有一种套路，就是偏差大，欠拟合那就模型复杂化，多来点特征，多上几层。方差大那就是过拟合，那就多来点数据，来点正则，去掉一些不重要的特征，神经网络的话就加上dropout。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;目标：定义什么样的损失函数来作为优化的目标,常用&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;最小二乘OLS也称为平方损失&lt;/strong&gt;：&lt;script type=&quot;math/tex&quot;&gt;L(Y,f(x)) = (Y-f(x))^2&lt;/script&gt;，常见模型比如线性回归&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;最大似然（取上对数就是对数似然）&lt;/strong&gt;：&lt;script type=&quot;math/tex&quot;&gt;L(P,P(Y\mid X)) = -log(P(Y\mid X))&lt;/script&gt; 常见模型比如LR，Softmax&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Hinge-Loss&lt;/strong&gt;: &lt;script type=&quot;math/tex&quot;&gt;max(0,1-Y*f_\theta(X))&lt;/script&gt; 常见模型比如SVM&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;指数&lt;/strong&gt;: &lt;script type=&quot;math/tex&quot;&gt;exp(Y*f_\theta(X))&lt;/script&gt;,常见的模型比如Adaboost，GBDT&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;0，1损失&lt;/strong&gt;：比如分类模型LR&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;算法&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;最小二乘估计&lt;/strong&gt;：对不同的参数求导=0，用方程直接求出参数，这种方式适合数据量比较小的情况。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;BGD,SDG,MBGD&lt;/strong&gt;：适合数据量比较大的情况，使用梯度下降方式去逐渐逼近目标的最小值或者最大值。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;条件概率极大似然推导到非条件最小二乘&quot;&gt;条件概率（极大似然）推导到非条件（最小二乘）&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;结论&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;两者是等价。也就是两者目标在服从正态分布的情况下是可以发现目标是一致的&lt;/li&gt;
      &lt;li&gt;既然目标是一致的，那么算法也可以通过，也就是可用最小二乘估计来计算LR的似然目标，但一般不这么做。OLS不善于做0/1回归，目标值太小，毕竟不是普通的连续Y值的回归。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;推理&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;最大似然：假设Y值服从正态分布 &lt;script type=&quot;math/tex&quot;&gt;Y_i \approx N(f(x_i),\sigma^2)&lt;/script&gt; 且是独立分布,写成似然函数，就是 &lt;script type=&quot;math/tex&quot;&gt;P(Y_i\mid X_i) \approx exp(Y_i - f(X_i))^2&lt;/script&gt; 我这里省略了正态分布的其他部分，公式在电脑上是在敲的费劲啊~&lt;/li&gt;
      &lt;li&gt;由于概率独立分布，把所有Yi的概率相乘，取对数后求最大值，就会变成&lt;script type=&quot;math/tex&quot;&gt;min(\Sigma (Y_i - f(X_i))^2)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;怎么样，现在看着就是最小二乘的公式了吧&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lr的似然函数和最小二乘的关系&quot;&gt;LR的似然函数和最小二乘的关系&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;怎么LR的推导不出来？不是正态分布嘛，当然推导不出来。
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;LR的似然 &lt;script type=&quot;math/tex&quot;&gt;P(Y_i\mid X_i) = h_\theta (x_i)^{Y_i} *  (1-h_\theta (x_i))^{(1-Y_i)}&lt;/script&gt; 同样可以取LOG后相加，就是所有概率的相乘啦。&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;LR的似然为啥和最小二乘那么不同呢，好像推导不出来，没有关系，我先声明了，Y要服从正态分布，可是LR的Y的0 1分布，很显然是bernoulli分布 &lt;script type=&quot;math/tex&quot;&gt;y \approx Bernoulli(\phi)&lt;/script&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;NG说，你看他们求导后，梯度怎么看着是一模一样的，哈哈，确实，但 &lt;script type=&quot;math/tex&quot;&gt;\theta_i -=（f(x^{(j)})-y^{(j)})x_i^{(j)}&lt;/script&gt; 中的f函数是不同的哈，LR中的是有sigmoid非线性变换的。&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;后记：对于算法，我自己写了一些模型的基础实现，方便更清楚给同学解释 LR，SVM，C4.5，NN，NB是怎么回事哈，待我整理好了再发，写一篇文章不留神3个小时过去了~~~啊啊啊&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/24900876/answer/65176508&quot;&gt;最小二乘、极大似然、梯度下降有何区别？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/xidianzhimeng/article/details/20847289&quot;&gt;最大似然估计和最小二乘估计的区别与联系&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Sat, 11 Nov 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/11/11/olsmle-relation.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/11/11/olsmle-relation.html</guid>
        
        <category>machine learning</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>大数据技术漫谈：数据一致性那点事儿</title>
        <description>&lt;p&gt;AI时代公司要胜出，拼算法和数据。算法就那么些经典的可以挖掘，所以最宝贵的可能就是数据资源了，也就是前几年满大街的大数据思维。大数据有短时间过热的嫌疑，但从长远来看，未来是属于大数据的。最近零零碎碎的看了一些大数据相关资料，结合自己的理解，来谈谈大数据相关一些零星技术思考，本篇就从一致性技术谈起吧。写的有点杂乱，但还是分3点说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一致性起源：设计原则CAP, BASE, ACID&lt;/li&gt;
  &lt;li&gt;一致性保证：Paxos，锁&lt;/li&gt;
  &lt;li&gt;相关小细节：死锁，回调，MR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下一篇开始将从体系结构分别说一下大数据系统的协调，通信，数据通道，存储，以及计算。&lt;/p&gt;

&lt;h2 id=&quot;1一致性起源&quot;&gt;1.一致性起源&lt;/h2&gt;
&lt;p&gt;随着需要处理的数据越来越大，我们需要多台机器并行处理。那么数据分布式和服务分布式就比较重要了。为了保证我们的服务和数据的可用性，大牛们设计了一些原则，最顶层的就是&lt;strong&gt;CAP(Consistency,Availability,Partition Tolerance)&lt;/strong&gt;,说白了就是要在保证网络分区（服务的部分机房断网导致该部分通信中断）的情况下在数据一致性和服务可用性做一定的取舍。当然单机服务不用考虑P了。
在这样的框架有两种模式，一种是&lt;strong&gt;BASE(Bascialy available, Soft state, Eventually Consistency)&lt;/strong&gt;即为现在大部分强调可用性的分布式系统也就是保证P的情况下强调A，损失一部分C。另一种即为 &lt;strong&gt;ACID(Atomicity, Consistency, Isolation, Duration)&lt;/strong&gt;强调数据的完整一致，大部分的关系型数据库系统就是这么做的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据分布式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在多台机器上分布的存储数据。这里的数据，工业上讲就是我们所说SQL或者NoSQL数据库。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;SQL：常见的mysql,oracle,SQLserver等行存储数据库，扩展采用分库（不同结构数据）然后采用分表然后分区（相同结构数据），一旦采用这些技术后，势必导致表的查询或者连接麻烦，一般我们会有持久层中间件来实现这些技术负责路由找表如TDDL等。&lt;/li&gt;
  &lt;li&gt;NoSQL：常见的HDFS+HIVE形成离线数据库或者HBASE,MongoDB在线数据库。采用列存储方式，特别适合大规模稀疏性数据。天生就适合分布式，因为没有索引也不支持事务，以文件的形式存在。&lt;/li&gt;
  &lt;li&gt;优缺点：如果是那种全信息查询，比如profile之类的，使用行存储会比较好，如果是只取某一列信息，比如跟帖这类的且schema并不固定，使用列存储会比较好。&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;程序分布式
    &lt;ol&gt;
      &lt;li&gt;大规模离线计算：使用hadoop+mapreduce实现多台机器并行计算，mapreduce通过计算map数量，partition数据决定reduce数量，当jobtracker在maptask任务后进行shuffle（sort+combine），然后缓行相关的reduce任务进行数据获取。&lt;/li&gt;
      &lt;li&gt;大规模在线服务集群：kafka进行日志手机后，可以使用zookeeper来管理协条多台服务并发，而服务器与服务器之间通信可以使用消息队列，如MQ。具体细节不多说，大家可以参考张俊林的《大数据日知录》&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2一致性保证&quot;&gt;2.一致性保证&lt;/h2&gt;
&lt;p&gt;一致性我们从第一天写程序对数据处理，就会有接触到。这里我们从2个方面考虑一致性&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据
    &lt;ol&gt;
      &lt;li&gt;单机存储角度：保证数据是一致的，如果是文件，只有一份，当然就是一致的。如果是数据库，就得保证每次操作是符合ACID，注意不是CURD（增删改查），Innodb采用事务方式（锁）来保证原子性（要么成功要么失败），一致性（比如整体的约束不变），隔离性（事务锁机制），持久化（不会无缘无故回滚Rollback）。注意这里只是说的Innodb（在autocommit=0的情况下适合大量插入更新），而不是myisam(适合大量select）。&lt;/li&gt;
      &lt;li&gt;多机存储角度：SQL数据库则是主从配置多个备份,master负责写，slave负责读，master同步到slave。如果是文件nosql，则一般会有三处备份，其中一个是主备份。当主备份更新的时候，会通知其他备份更新。这里由于更新需要时间，所以不能保证强一致性。当然如果是大规模数据的话，会有哈希分片存储机制，如round-robin,一致性哈希技术来进行分布式存储。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;程序
    &lt;ol&gt;
      &lt;li&gt;（进程线程）数据库一致性操作：前面提到数据库问题，也就是每一条SQL语句都会默认成事务，那么就会取得锁，原则上为了保证数据不出现死锁，应该避免长时间的事务操作。增删改查尽量使用主键索引。&lt;/li&gt;
      &lt;li&gt;线程一致性操作：通常我们会使用synchornize来同步线程操作，线程对这个对象的操作前要先获得该对象的锁，如果synchornize()则默认是this，该对象，注意这里是排他锁X。如果要提高读写效率，可以使用Readlock和writelock。我们使用wait()进行对象锁的释放，notify（）完之后进行通知唤醒wait的线程重新开始竞争获取该对象的锁。。&lt;/li&gt;
      &lt;li&gt;进程通信：RPC，有很多种RPC框架，比如thrift，PB，dubbo等，通信格式有pb，thrift，json，xml等，经过他人相关实验对比看来，使用Thrift的TCompactprotocol无论从传统的压缩比还是cpu占用都是最优的。传输同样的数据比json压缩50%，cpu占用也差不多这个量级。&lt;img src=&quot;http://my.csdn.net/uploads/201208/13/1344830247_7145.png&quot; alt=&quot;说明图&quot; /&gt; &lt;img src=&quot;http://my.csdn.net/uploads/201208/13/1344830263_3364.png&quot; alt=&quot;cpu&quot; /&gt; 参考&lt;a href=&quot;http://blog.csdn.net/m13321169565/article/details/7835957#0-tsina-1-87119-397232819ff9a47a7b7e80a40613cfe1&quot;&gt;http://blog.csdn.net/m13321169565/article/details/7835957#0-tsina-1-87119-397232819ff9a47a7b7e80a40613cfe1&lt;/a&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3相关小细节&quot;&gt;3.相关小细节&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;死锁问题:之前做数据更新总是遇到死锁，简单来说就是对资源进行加锁，在没有释放锁的情况下继续申请锁（不管是两个线程申请同一个对象的锁，还是申请不同对象的锁）都有可能死锁。所以尽量用完就释放，不管是读锁还是写锁。&lt;/li&gt;
  &lt;li&gt;通信机制：服务器端sockect-bind-listen-accept-fork-recv-send, 客户端socket-[bind]-connect-send-recv。这是通用的模式，当然很多语言会有自己的包封。&lt;/li&gt;
  &lt;li&gt;回调消息:回调简单说来就是我让你做事，你做完了告诉我一声，怎么告诉我，我来定这个方案，这个方案就是回调函数。需要反馈的人负责实现，做事的你不用负责，只要知道就可以。同步就是等你做完事通知我之后我再做事，异步就是我接着做我的事不用等你通知我，你完事了告诉我就可以。是不是有点绕，哈哈，请参考我的源码&lt;a href=&quot;https://github.com/bobz653/programming/tree/master/callbackjava&quot;&gt;https://github.com/bobz653/programming/tree/master/callbackjava&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MR:jobtracker下面有多个tasktracker(map,reduce),tasktracker只能运行在DataNode上。map阶段会执行sort-merge这些shuffle动作，减少磁盘IO，然后通知jobtracker已经完成，jobtracker会告知reducetask来取数据，reduce会从各个map节点拉去已经merge好的数据再次reduce合并，输出结果.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4参考&quot;&gt;4.参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;死锁: &lt;a href=&quot;http://www.jb51.net/article/32651.htm&quot;&gt;http://www.jb51.net/article/32651.htm&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;MR: &lt;a href=&quot;http://blog.csdn.net/yczws1/article/details/19178265#0-tsina-1-2883-397232819ff9a47a7b7e80a40613cfe1&quot;&gt;http://blog.csdn.net/yczws1/article/details/19178265#0-tsina-1-2883-397232819ff9a47a7b7e80a40613cfe1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;同步、异步回调：&lt;a href=&quot;http://www.cnblogs.com/puyangsky/p/5581051.html&quot;&gt;http://www.cnblogs.com/puyangsky/p/5581051.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 01 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E5%A4%A7%E6%95%B0%E6%8D%AE/2017/07/01/big-data-1.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E5%A4%A7%E6%95%B0%E6%8D%AE/2017/07/01/big-data-1.html</guid>
        
        <category>一致性 死锁 设计原则</category>
        
        
        <category>大数据</category>
        
      </item>
    
      <item>
        <title>浅析RNN(LSTM)网络结构【附源码】</title>
        <description>&lt;p&gt;前一篇博文讨论了深度学习基础和一些数学知识，比如&lt;strong&gt;Word2vec&lt;/strong&gt;，为啥要选mikolov的word2vec，虽然这个模型还不是深度网络，但包含了多次LR的参数训练，这个过程我觉得就是所有神经网络的算法基础。目前深度学习用的比较多的是CNN，RNN。在图像中用CNN比较多，因为CNN中的filter+pooling可以在图片各种位置空间上学习出多种多样的特征。在时序信息（&lt;em&gt;信息是有关时间和序列的&lt;/em&gt;）中用的比较多的是RNN，比如语音，NLP，以及时序信息。LSTM是RNN中的用的比较多的网络，看了许多资料现在算是明白一些了。本文也分3个部分，最后会附上样例代码，现在开始从底层分析LSTM的结构吧。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型分析：RNN到LSTM，结构和参数分析&lt;/li&gt;
  &lt;li&gt;模型应用：预测（分类、序列Seq2Seq）&lt;/li&gt;
  &lt;li&gt;安装和源码：注意的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;模型分析&quot;&gt;模型分析&lt;/h2&gt;
&lt;p&gt;Word2vec的无监督学习可以学习到当前词周边的语义，似然函数是&lt;script type=&quot;math/tex&quot;&gt;P(W_i \| context_wi) = -log((&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{ exp (W^tx)} \over {\Sigma^V_1 exp(W^ix)}&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;))&lt;/script&gt;,损失函数使用交叉熵，使得似然函数最大，当然这里似然函数取了负值。当然说是无监督学习，里面用的是监督。这里在训练的时候参数不断的更新，其实这已经像RNN了，上次的参数可以继承。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN：在RNN序列信息学习中，目标是分类或者是序列。这里训练数据需要上一时刻的隐藏层参数&lt;script type=&quot;math/tex&quot;&gt;HiddenLayer_t&lt;/script&gt; 即为 &lt;script type=&quot;math/tex&quot;&gt;H_t&lt;/script&gt;，注意这个参数是列向量，里面是数值类型。这里向量的合并可以理解成相加，类似word2vec那样。&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png&quot; alt=&quot;RNN结构图&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;LSTM：RNN的问题就在于直接进行状态相加，太粗暴，变化也太剧烈，容易把之前的状态覆盖掉。那么我们可不可以改变RNN这种粗暴的方式，把状态记住呢，这里就有了long term memory,有了存储这个向量的空间，同事我们训练的时候，从个里面拿出有用的memory和当前的memory构成short memory传输给输出层，或者下一层，如下图：&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png&quot; alt=&quot;LSTM结构图&quot; /&gt;，需要注意的是图中的叉是point-wise点乘，不是点积。这里有4个参数运算要进行，从左边到右边，依次是
    &lt;ol&gt;
      &lt;li&gt;第一个参数运算&lt;strong&gt;Forget gate:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;f_t = \sigma(W_f(h_{t-1},X_t)+bias)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;第二，三个参数运算&lt;strong&gt;Input gate &amp;amp; candidate memory:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;f_i = \sigma(W_i(h_{t-1},X_t)+bias) * tanh(W_c(h_{t-1},X_t)+bias)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;形成长期记忆&lt;strong&gt;Long term meory&lt;/strong&gt;： &lt;script type=&quot;math/tex&quot;&gt;C_t = f_t + f_i&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;第四个参数运算&lt;strong&gt;Ouput Gate&lt;/strong&gt;输出到下一层： &lt;script type=&quot;math/tex&quot;&gt;h_t = \sigma(W_o(h_{t-1},X_t)+bias) * tanh(C_t)&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整的模型分析就是这四步。可以通过链式求导推导出参数更新，和LR并无不同，比较繁琐具体的推导过程就不写了。&lt;/p&gt;

&lt;h2 id=&quot;模型应用&quot;&gt;模型应用&lt;/h2&gt;
&lt;p&gt;在机器学习中，只有2种任务，预测任务（分类，回归）、描述任务（聚类，关联模式，异常检测）。我们LSTM经常用来解决分类或者回归任务，常用的分类任务，比如情感分类，回归任务比如速度预测等，LSTM通过是作用底层的语义向量层，然后上面会级联分类模型（LR、softmax），或者回归模型（广义回归（LR））。模式都差不多，对于LSTM来说&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果最后用的是LSTM比如历史N个时刻的向量做Max pooling 或者mean pooling,一般这是分类任务。&lt;img src=&quot;http://www.th7.cn/d/file/p/2016/02/15/36970b182f7ce34b223ac79443fafdb2.jpg&quot; alt=&quot;分类图&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;如果最后用的是每个LSTM时刻的向量，通常这是序列标注任务。&lt;img src=&quot;http://pic.w2bc.com/upload/201703/10/201703101724089047.jpg&quot; alt=&quot;序列标注图&quot; /&gt; 这是就类似咱们通常说的&lt;strong&gt;Seq2Seq&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;安装和源码&quot;&gt;安装和源码&lt;/h2&gt;
&lt;p&gt;网上有很多相关的源码，很容易获取，这里我给个基本安装配置,源码是做情感分类。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;版本兼容问题
    &lt;blockquote&gt;
      &lt;pre&gt;import keras 
print keras.__version__
import tensorflow
print tensorflow.__version__
&lt;/pre&gt;
      &lt;p&gt;我机器上的keras版本是2.0.4,tensorflow版本是1.1.0，注意keras默认的后端是TensorFlow，因为这个作者本来也是tensorflow的作者，所以当然用自己的东西啦，Theano目前来看已经式微了。版本一定使用最新的，不然会出现model.add()的时候报错哦。如果要安装新的记得&lt;/p&gt;
      &lt;pre&gt;pip uninstall keras/tensorflow&lt;/pre&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;网络参数个数
    &lt;blockquote&gt;
      &lt;p&gt;代码中通过model.summary()可以看到参数数量的多少，我这里可以给大家算一下，这样基本就知道网络模型是个什么样子。这里有词汇库14322个，embedding层是256维度，那么embeddign参数是14322*256,LSTM层的参数是4次计算，每次有个偏置，还有连接层的参数，所以参数为256 * 128 + 128 * 129 = 197120.明白参数的个数对模型了解很重要。&lt;/p&gt;
      &lt;pre&gt;model = Sequential()
model.add(Embedding(14322, 256, input_length=maxlen))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
optimizer='adam',
metrics=['accuracy'])&lt;/pre&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.运算时间&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;目前15000个句子，词汇量是14322，batchsize为128，我机器比较弱，一次epoch大概需要3分钟，30次epoch训练完毕需要1个半小时。我没有GPU，所以比较慢了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;源码见我的GITHUB：&lt;a href=&quot;https://github.com/bobz653/machine-learning/tree/master/emotion_category_lstm&quot;&gt;https://github.com/bobz653/machine-learning/tree/master/emotion_category_lstm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PS： &lt;strong&gt;要指出的是网络上有篇传播很广的&lt;a href=&quot;http://blog.csdn.net/Dark_Scope/article/details/47056361&quot;&gt;博文&lt;/a&gt;，里面提到了LSTM的结构，说的相当不清楚，大家不要去看。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;LSTM相关文章：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jianshu.com/p/9dc9f41f0b29&quot;&gt;http://www.jianshu.com/p/9dc9f41f0b29&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.echen.me/2017/05/30/exploring-lstms/&quot;&gt;http://blog.echen.me/2017/05/30/exploring-lstms/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kexue.fm/archives/3863/&quot;&gt;http://kexue.fm/archives/3863/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/malefactor/article/details/50725480&quot;&gt;http://blog.csdn.net/malefactor/article/details/50725480&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 11 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/06/11/lstm-basic.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/06/11/lstm-basic.html</guid>
        
        <category>machine learning</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>从word2vec源码讨论深度学习基础</title>
        <description>&lt;p&gt;最近AlphaGo战胜了柯洁让人工智能又火了一把，BAT的老大们（据说分类为&lt;em&gt;技术男，文科生，产品经理&lt;/em&gt;）在讨论，大牛Jordan说AI过火了。所有的讨论狗来说都算是胜利。建立在神经网络结构上的机器学习算法确实比以往的算法表达能力更为广泛，谷歌的单&lt;strong&gt;TPU2&lt;/strong&gt;结构可以达到45 TFLOPS，这种专门为Tensor矩阵计算做出的ASIC集成芯片，相比如NVIDIA的volta的120TFLOPS的运算速度虽然差一些，但成本据说便宜了近90%。抛开业界如火如荼的产品和发布会不谈，拨开五颜六色的产品，从最底层了解深度学习的技术是很有意思的一件事情，让我们就从13年mikolov发布的训练词向量的模型word2vec入手吧，我分成3点来讲讲这个模型。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数学基础&lt;/li&gt;
  &lt;li&gt;模型基础&lt;/li&gt;
  &lt;li&gt;词向量&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;数学基础&quot;&gt;数学基础&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;线性方程： &lt;script type=&quot;math/tex&quot;&gt;y = wx + b&lt;/script&gt; 就是常用的线性模型，当y时离散变量的时候则是分类问题，当y时连续变量的时候，就是回归问题了。&lt;/li&gt;
  &lt;li&gt;最小二乘求参数：通常对线性方程的参数求解，理想情况下可以用最小二乘法直接求解 &lt;script type=&quot;math/tex&quot;&gt;X\Theta = y&lt;/script&gt; 对方阵才能求逆矩阵，所以两边都乘以 &lt;script type=&quot;math/tex&quot;&gt;X^T&lt;/script&gt;,然后两边乘以逆矩阵，则解： &lt;script type=&quot;math/tex&quot;&gt;\Theta = (X^tX)^{-1}X^Ty&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;非线性方程：大名鼎鼎的&lt;strong&gt;Sigmoid，Tanh，Relu&lt;/strong&gt;。为了避免梯度消失或者爆炸，目前大家基本都用Relu。这里不做过多介绍。&lt;/li&gt;
  &lt;li&gt;梯度下降求参数：设代价函数为&lt;script type=&quot;math/tex&quot;&gt;J(\Theta)&lt;/script&gt;, 让代价函数最小的方法是对J函数参数求导，某个参数要更新就是使用下面的公式 &lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;{\partial J(\Theta)} \over {\partial\Theta_j}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;参数解的个数：要么无解，要么1个或者无数个。&lt;/li&gt;
  &lt;li&gt;Huffman编码：类似层次聚类，选择数量最小的2个，两两合并，形成一个二叉树，规定左子树为0，右子树为1。则每个叶子节点的路径用01序列表示，这样做的好处就是编码长度最小，因为数量高的一定靠近根节点处，编码短。&lt;/li&gt;
  &lt;li&gt;特征值和特征向量：所谓特征值是指所有样本在特征向量（单位向量）的投影的方差，特征值越大，信息量越大。这里的特征向量可以理解成X，Y，Z轴这样的正交向量。两个向量的点积A.B就可以理解成A向量在B向量上的投影。&lt;/li&gt;
  &lt;li&gt;矩阵分解：常用的矩阵分解有&lt;strong&gt;PCA，SVD，NMF&lt;/strong&gt;几种，所谓矩阵分解就是要从矩阵中找点隐藏的信息，比如主题什么的。比如&lt;script type=&quot;math/tex&quot;&gt;A_{m*n} = U_{m*k}V_{k*n}&lt;/script&gt; 其中m是样本数，k是主题数，n物品数。矩阵PCA分解的一般步骤：
    &lt;ol&gt;
      &lt;li&gt;减去均值&lt;/li&gt;
      &lt;li&gt;求矩阵的协方差&lt;script type=&quot;math/tex&quot;&gt;\sum_{n*n}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;求协方差的特征值和特征向量，这里的协方差矩阵&lt;script type=&quot;math/tex&quot;&gt;\sum_{n*n}&lt;/script&gt;的TopK特征值就是我们要的K个主题，这里特征向量矩阵即为&lt;script type=&quot;math/tex&quot;&gt;V_{n*k}&lt;/script&gt;。&lt;/li&gt;
      &lt;li&gt;原始矩阵压缩到K维度即为&lt;script type=&quot;math/tex&quot;&gt;A_{m*n}* V_{n*k}&lt;/script&gt;,矩阵存储空间&lt;script type=&quot;math/tex&quot;&gt;m*n 变成m*k+ k*n&lt;/script&gt;. SVD和NMF类同。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;模型基础&quot;&gt;模型基础&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;逻辑回归：逻辑回归当y=1的时候损失为&lt;script type=&quot;math/tex&quot;&gt;-logh_\theta(X)&lt;/script&gt; 也就是&lt;script type=&quot;math/tex&quot;&gt;h_\theta(X)&lt;/script&gt;越接近1，损失越小，当y=0的时候损失则为&lt;script type=&quot;math/tex&quot;&gt;-log(1-h_\theta(X))&lt;/script&gt;。合并在一起，则损失函数为 &lt;script type=&quot;math/tex&quot;&gt;-1/m[\sum_{i=1}^{m}y^{(i)}*logh_\theta(X^{(i)}) + (1-y^{(i)})log(1-h_\theta(X^{(i)}))]&lt;/script&gt; 分类问题通常使用逻辑回归或者softmax解决问题：使用sigmoid函数 &lt;script type=&quot;math/tex&quot;&gt;\sigma(x)=&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{ 1 } \over {1+exp(-h_{w,b}(x))}&lt;/script&gt; 将线性函数强行掰弯，值域在(0,1)之间。如果样本均匀分布则可以设置阈值为0.5.如下图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://p.blog.csdn.net/images/p_blog_csdn_net/chl033/612813/o_SigmoidFunction_701_2.gif&quot; alt=&quot;图示&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;模型求解：使用梯度下降，如果是随机梯度下降SGD：&lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta(y^{(i)} - h_\Theta(x^{(i)}))X_j^{(i)}&lt;/script&gt; ，批量梯度下降MGD则为 &lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta 1 / m\sum_{i=1}^{i=m}(y^{(i)} - h_\Theta(x^{(i)}))X_j^{(i)}&lt;/script&gt;  有意思的最小二乘和逻辑回归使用梯度下降的公式是一样的，这里NG的解释是，嗯，虽然形式一样，但内容不用，里面的y值sigmoid是非线性而在最小二乘中是线性的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;正向传播，反向传播：逻辑回归中其实已经用到了一次反向传播更新参数，当网络有多层的时候，比如MLP神经网络，这里就要链式求导。比如网络比逻辑回归多一层，也就是隐藏层，那么正向传播的公式：&lt;script type=&quot;math/tex&quot;&gt;y=h_\theta(g_w(X))&lt;/script&gt; ，反向传播的公式为：&lt;script type=&quot;math/tex&quot;&gt;W_j:=W_j - \eta *error*h(1-h)\theta*g*(1-g)X_j^{(i)}&lt;/script&gt;,此处可以看到sigmoid的问题就在层数越多，梯度越小，最后梯度就消失了，使用Relu可以避免这个问题，因为Relu的梯度是1，不增也不减。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;词向量&quot;&gt;词向量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;问题起因：数据挖掘（数据分析，数据清洗，特征处理，模型验证，模型集成）其中特征处理有一项，就是可以给特征做embedding。以前我们做labelencoder及one-hotencoder，虽然可以对category变量进行编码，方便模型进行计算比如NB，SVM，LR，KNN等，前提是特征独立，但大多数情况，特征并不独立，比如“麦克”和“话筒”。如果one-hot化之后，他们之间的相似度是0.&lt;/li&gt;
  &lt;li&gt;解决思路：词周围的词决定了当前词的意义，那么我们可否用周围词出现的数量来标记当前词呢，当然是可以的。Globalvec就是这么做的。但数量的刻画毕竟太过简单，是否有一个更好的拟合函数来刻画&lt;script type=&quot;math/tex&quot;&gt;P(W_i \|Context_{W_i})&lt;/script&gt;呢？当然也是可以的，这里就用到了一层感知机模型，用无指导的方式，训练，最大似然该拟合函数。实际上就是多个LR模型的训练，参考 &lt;img src=&quot;http://img.blog.csdn.net/20140525173342578&quot; alt=&quot;CBOW+HS&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;源码分析：目前我们只分析CBOW+HS模式，其他模式比如SkipGram+NS是类似的。
    &lt;ol&gt;
      &lt;li&gt;初始化：比如设置向量维度是50维度，输入Syn0向量保存了词汇库所有不重复词V的向量，初始化不为零的小数。Syn1存放huffman树的非叶节点参数，也是50维度，初始设置为0。窗口可以设置为5。&lt;/li&gt;
      &lt;li&gt;训练：&lt;img src=&quot;http://img.blog.csdn.net/20140525182637750&quot; alt=&quot;源码1&quot; /&gt; &lt;img src=&quot;http://img.blog.csdn.net/20140525182645906&quot; alt=&quot;源码2&quot; /&gt; 当一句读进来，每一个词W都会有上下文向量（不包括自己），将50维向量累加，形成向量nelu1,对于W的huffman树有个编码，比如1100，那么将执行4词逻辑回归，每次逻辑回归，都会按照公式更新参数，Syn1参数更新公式&lt;script type=&quot;math/tex&quot;&gt;Syn1[i]:=Syn1[i] - \eta(y - nelu1*syn1)*nelu1&lt;/script&gt; 这里y是树编码的某个位的数值，累计残差为 &lt;script type=&quot;math/tex&quot;&gt;$nelue += \eta(y - nelu1*syn1)* syn1&lt;/script&gt; 然后更新Syn0，注意不是更新nelu1,而是更新Syn0 &lt;script type=&quot;math/tex&quot;&gt;Syn0 += nelue&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;结束：一轮迭代2G文本，大概10亿字，训练2-3小时即可。由于word2vec并不是神经网络，不含有隐层，所以做了N次逻辑回归，反向传播就一次，速度是非常快的。训练完毕，每个词向量就有了，使用PCA进行降维，可以把一些有意思的词可视化出来。对于向量的可视化，可以参考multidimensional scaling，也挺有意思的。&lt;/li&gt;
      &lt;li&gt;有关算法提速：通过hs加速训练，NS也可以。另外exp指数计算采用的查表方式，NS和HS其实可以混合使用。另外&lt;strong&gt;多线程并没有加锁&lt;/strong&gt;，这个问过作者，说多次梯度下降没有问题，就算重复更新也无妨。有兴趣的可以参考&lt;a href=&quot;http://blog.csdn.net/mytestmy/article/details/26961315&quot;&gt;博文&lt;/a&gt;，这位博主写的挺好。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;学着使用mathjax，公式写的有点仓促，但挺好用的哈，参考&lt;a href=&quot;http://m.blog.csdn.net/article/details?id=46757757&quot;&gt;http://m.blog.csdn.net/article/details?id=46757757&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;word2vec写的不错的博客地址&lt;a href=&quot;http://blog.csdn.net/mytestmy/article/details/26969149&quot;&gt;http://blog.csdn.net/mytestmy/article/details/26969149&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PCA、SVD的内容参考&lt;a href=&quot;http://blog.sina.com.cn/s/blog_66a6172c0102v1v7.html&quot;&gt;http://blog.sina.com.cn/s/blog_66a6172c0102v1v7.html&lt;/a&gt;,写的比较通俗&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 30 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/05/30/from-word2vec.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/05/30/from-word2vec.html</guid>
        
        <category>machine learning</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>利用Jekyll在Github搭建博客</title>
        <description>&lt;p&gt;这周末刚好项目不那么忙，抽空利用jekyll在本地上搭建博客，然后推送到自己的&lt;a href=&quot;https://bobz653.github.io&quot; title=&quot;博客&quot;&gt;Github博客&lt;/a&gt;中，对日常中好奇，好玩，复习的知识进行记录，也分享给感兴趣的朋友。&lt;/p&gt;

&lt;h2 id=&quot;1环境准备&quot;&gt;1.环境准备&lt;/h2&gt;
&lt;p&gt;我是在mac中搭建的，所以其他环境请找相应的安装包。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.安装Jekyll：使用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
gem -v;
gem update --system;
sudo gem install jekyll -n /usr/local/Cellar/;
jekyll -v;
&lt;/pre&gt;
&lt;p&gt;如果可以正常显示版本信息，说明安装成功。&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;2.github 博客：在&lt;a href=&quot;https://github.com&quot;&gt;Github&lt;/a&gt;注册账户&lt;/p&gt;

  &lt;p&gt;新建一个以用户名.github.io为名称的工程即可，这个工程有且仅有一个，是github使用gitpages服务托管的仅支持jekyll这样的静态页面的服务。在本地使用ssh-genkey创建rsa_pub文件，加入到git的ssh pubkey中即可通过&lt;/p&gt;
  &lt;pre&gt;git clone;
git push origin master
&lt;/pre&gt;
  &lt;p&gt;等命令在本地博客更新后，来更新线上博客了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2搭建博客&quot;&gt;2.搭建博客&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;1.合适的模板：在jeklly中有很多免费的模板供大家使用，比如&lt;a href=&quot;https://jekyllthemes.io/&quot;&gt;Jekyll theme&lt;/a&gt;，找到一个就git clone下来吧。&lt;/p&gt;

  &lt;p&gt;2.本地运行：&lt;/p&gt;
  &lt;pre&gt;
jekyll build;
jekyll server;
&lt;/pre&gt;
  &lt;p&gt;使用http://127.0.0.1:4000/即可在本地查看。&lt;/p&gt;

  &lt;p&gt;3.修改：&lt;/p&gt;

  &lt;p&gt;文章在_post文件夹中，全局配置在_config.yml中，css在_sass中,布局在_layout中。修改后所有的网站最终文件在_site文件夹中。&lt;/p&gt;

  &lt;p&gt;新建的文章都在_post文件夹中，以YYYY-MM-DD-title.md方式存在。且头部是一些模板需要读取的信息，比如作者，类别，标题，标签等信息。可以用rake命令自动生成，定义好Rakefile之后，使用&lt;/p&gt;
  &lt;pre&gt;rake post title='hello world' &lt;/pre&gt;
  &lt;p&gt;这样的命令即可。
4.针对公式编辑在概率，算法中常常用到，我们可以使用mathjax来进行。&lt;/p&gt;

  &lt;pre&gt; &lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;
formula1: $$n==x$$
formula2: $$n!=x$$
&lt;/pre&gt;
  &lt;p&gt;效果：
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;formula1: &lt;script type=&quot;math/tex&quot;&gt;n==x&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;formula2: &lt;script type=&quot;math/tex&quot;&gt;n!=x&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;5.当然也可以加一些分享，评论的JS到博客中，在_layout中可以修改post模板，以增加这些功能。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3推送到线上&quot;&gt;3.推送到线上&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;有前面的准备，这个步骤就简单了啊，直接使用&lt;/p&gt;
  &lt;pre&gt;
git add -A ;#这里是加入删除选项
git commit -m 'comments';
git push origin master;
&lt;/pre&gt;

  &lt;p&gt;然后访问https://youname.github.io 即可，可能有几秒的延迟。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 05 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9D%82%E8%B0%88/2017/05/05/hello-world.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9D%82%E8%B0%88/2017/05/05/hello-world.html</guid>
        
        <category>jekyll github</category>
        
        
        <category>杂谈</category>
        
      </item>
    
  </channel>
</rss>
