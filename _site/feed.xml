<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>探索发现好玩的东西</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 11 Jun 2017 21:16:39 +0800</pubDate>
    <lastBuildDate>Sun, 11 Jun 2017 21:16:39 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>浅析RNN(LSTM)网络结构【附源码】</title>
        <description>&lt;p&gt;前一篇博文讨论了深度学习基础和一些数学知识，比如&lt;strong&gt;Word2vec&lt;/strong&gt;，为啥要选mikolov的word2vec，虽然这个模型还不是深度网络，但包含了多次LR的参数训练，这个过程我觉得就是所有神经网络的算法基础。目前深度学习用的比较多的是CNN，RNN。在图像中用CNN比较多，因为CNN中的filter+pooling可以在图片各种位置空间上学习出多种多样的特征。在时序信息（&lt;em&gt;信息是有关时间和序列的&lt;/em&gt;）中用的比较多的是RNN，比如语音，NLP，以及时序信息。LSTM是RNN中的用的比较多的网络，看了许多资料现在算是明白一些了。本文也分3个部分，最后会附上样例代码，现在开始从底层分析LSTM的结构吧。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型分析：RNN到LSTM，结构和参数分析&lt;/li&gt;
  &lt;li&gt;模型应用：预测（分类、序列Seq2Seq）&lt;/li&gt;
  &lt;li&gt;安装和源码：注意的问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;模型分析&quot;&gt;模型分析&lt;/h2&gt;
&lt;p&gt;Word2vec的无监督学习可以学习到当前词周边的语义，似然函数是&lt;script type=&quot;math/tex&quot;&gt;P(W_i \| context_wi) = -log((&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{ exp (W^tx)} \over {\Sigma^V_1 exp(W^ix)}&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;))&lt;/script&gt;,损失函数使用交叉熵，使得似然函数最大，当然这里似然函数取了负值。当然说是无监督学习，里面用的是监督。这里在训练的时候参数不断的更新，其实这已经像RNN了，上次的参数可以继承。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN：在RNN序列信息学习中，目标是分类或者是序列。这里训练数据需要上一时刻的隐藏层参数&lt;script type=&quot;math/tex&quot;&gt;HiddenLayer_t&lt;/script&gt; 即为 &lt;script type=&quot;math/tex&quot;&gt;H_t&lt;/script&gt;，注意这个参数是列向量，里面是数值类型。这里向量的合并可以理解成相加，类似word2vec那样。&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png&quot; alt=&quot;RNN结构图&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;LSTM：RNN的问题就在于直接进行状态相加，太粗暴，变化也太剧烈，容易把之前的状态覆盖掉。那么我们可不可以改变RNN这种粗暴的方式，把状态记住呢，这里就有了long term memory,有了存储这个向量的空间，同事我们训练的时候，从个里面拿出有用的memory和当前的memory构成short memory传输给输出层，或者下一层，如下图：&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png&quot; alt=&quot;LSTM结构图&quot; /&gt;，需要注意的是图中的叉是point-wise点乘，不是点积。这里有4个参数运算要进行，从左边到右边，依次是
    &lt;ol&gt;
      &lt;li&gt;第一个参数运算&lt;strong&gt;Forget gate:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;f_t = \sigma(W_f(h_{t-1},X_t)+bias)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;第二，三个参数运算&lt;strong&gt;Input gate &amp;amp; candidate memory:&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;f_i = \sigma(W_i(h_{t-1},X_t)+bias) * tanh(W_c(h_{t-1},X_t)+bias)&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;形成长期记忆&lt;strong&gt;Long term meory&lt;/strong&gt;： &lt;script type=&quot;math/tex&quot;&gt;C_t = f_t + f_i&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;第四个参数运算&lt;strong&gt;Ouput Gate&lt;/strong&gt;输出到下一层： &lt;script type=&quot;math/tex&quot;&gt;h_t = \sigma(W_o(h_{t-1},X_t)+bias) * tanh(C_t)&lt;/script&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整的模型分析就是这四步。可以通过链式求导推导出参数更新，和LR并无不同，比较繁琐具体的推导过程就不写了。&lt;/p&gt;

&lt;h2 id=&quot;模型应用&quot;&gt;模型应用&lt;/h2&gt;
&lt;p&gt;在机器学习中，只有2种任务，预测任务（分类，回归）、描述任务（聚类，关联模式，异常检测）。我们LSTM经常用来解决分类或者回归任务，常用的分类任务，比如情感分类，回归任务比如速度预测等，LSTM通过是作用底层的语义向量层，然后上面会级联分类模型（LR、softmax），或者回归模型（广义回归（LR））。模式都差不多，对于LSTM来说&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果最后用的是LSTM比如历史N个时刻的向量做Max pooling 或者mean pooling,一般这是分类任务。&lt;img src=&quot;http://www.th7.cn/d/file/p/2016/02/15/36970b182f7ce34b223ac79443fafdb2.jpg&quot; alt=&quot;分类图&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;如果最后用的是每个LSTM时刻的向量，通常这是序列标注任务。&lt;img src=&quot;http://pic.w2bc.com/upload/201703/10/201703101724089047.jpg&quot; alt=&quot;序列标注图&quot; /&gt; 这是就类似咱们通常说的&lt;strong&gt;Seq2Seq&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;安装和源码&quot;&gt;安装和源码&lt;/h2&gt;
&lt;p&gt;网上有很多相关的源码，很容易获取，这里我给个基本安装配置&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;版本兼容问题
    &lt;blockquote&gt;
      &lt;pre&gt;import keras 
print keras.__version__
import tensorflow
print tensorflow.__version__
&lt;/pre&gt;
      &lt;p&gt;我机器上的keras版本是2.0.4,tensorflow版本是1.1.0，注意keras默认的后端是TensorFlow，因为这个作者本来也是tensorflow的作者，所以当然用自己的东西啦，Theano目前来看已经式微了。版本一定使用最新的，不然会出现model.add()的时候报错哦。&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;网络参数个数
    &lt;blockquote&gt;
      &lt;p&gt;代码中通过model.summay()可以看到参数数量的多少，我这里可以给大家算一下，这样基本就知道网络模型是个什么样子。这里有词汇库14322个，embedding层是256维度，那么embeddign参数是14322*256,LSTM层的参数是4次计算，每次有个偏置，还有连接层的参数，所以参数为256 * 128 + 128 * 129 = 197120.明白参数的个数对模型了解很重要。&lt;/p&gt;
      &lt;pre&gt;model = Sequential()
model.add(Embedding(14322, 256, input_length=maxlen))
model.add(LSTM(128))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
optimizer='adam',
metrics=['accuracy'])&lt;/pre&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3.运算时间&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;目前15000个句子，词汇量是14322，batchsize为128，我机器比较弱，一次epoch大概需要3分钟，30次epoch训练完毕需要1个半小时。我没有GPU，所以比较慢了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;源码见我的GITHUB：&lt;a href=&quot;https://github.com/bobz653/machine-learning/tree/master/emotion_category_lstm&quot;&gt;https://github.com/bobz653/machine-learning/tree/master/emotion_category_lstm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;PS： &lt;strong&gt;要指出的是网络上有篇传播很广的&lt;a href=&quot;http://blog.csdn.net/Dark_Scope/article/details/47056361&quot;&gt;博文&lt;/a&gt;，里面提到了LSTM的结构，说的相当不清楚，大家不要去看。&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;LSTM相关文章：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jianshu.com/p/9dc9f41f0b29&quot;&gt;http://www.jianshu.com/p/9dc9f41f0b29&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.echen.me/2017/05/30/exploring-lstms/&quot;&gt;http://blog.echen.me/2017/05/30/exploring-lstms/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://kexue.fm/archives/3863/&quot;&gt;http://kexue.fm/archives/3863/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/malefactor/article/details/50725480&quot;&gt;http://blog.csdn.net/malefactor/article/details/50725480&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 11 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/06/11/lstm-basic.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/06/11/lstm-basic.html</guid>
        
        <category>machine learning</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>从word2vec源码讨论深度学习基础</title>
        <description>&lt;p&gt;最近AlphaGo战胜了柯洁让人工智能又火了一把，BAT的老大们（据说分类为&lt;em&gt;技术男，文科生，产品经理&lt;/em&gt;）在讨论，大牛Jordan说AI过火了。所有的讨论狗来说都算是胜利。建立在神经网络结构上的机器学习算法确实比以往的算法表达能力更为广泛，谷歌的单&lt;strong&gt;TPU2&lt;/strong&gt;结构可以达到45 TFLOPS，这种专门为Tensor矩阵计算做出的ASIC集成芯片，相比如NVIDIA的volta的120TFLOPS的运算速度虽然差一些，但成本据说便宜了近90%。抛开业界如火如荼的产品和发布会不谈，拨开五颜六色的产品，从最底层了解深度学习的技术是很有意思的一件事情，让我们就从13年mikolov发布的训练词向量的模型word2vec入手吧，我分成3点来讲讲这个模型。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数学基础&lt;/li&gt;
  &lt;li&gt;模型基础&lt;/li&gt;
  &lt;li&gt;词向量&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;数学基础&quot;&gt;数学基础&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;线性方程： &lt;script type=&quot;math/tex&quot;&gt;y = wx + b&lt;/script&gt; 就是常用的线性模型，当y时离散变量的时候则是分类问题，当y时连续变量的时候，就是回归问题了。&lt;/li&gt;
  &lt;li&gt;最小二乘求参数：通常对线性方程的参数求解，理想情况下可以用最小二乘法直接求解 &lt;script type=&quot;math/tex&quot;&gt;X\Theta = y&lt;/script&gt; 对方阵才能求逆矩阵，所以两边都乘以 &lt;script type=&quot;math/tex&quot;&gt;X^T&lt;/script&gt;,然后两边乘以逆矩阵，则解： &lt;script type=&quot;math/tex&quot;&gt;\Theta = (X^tX)^{-1}X^Ty&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;非线性方程：大名鼎鼎的&lt;strong&gt;Sigmoid，Tanh，Relu&lt;/strong&gt;。为了避免梯度消失或者爆炸，目前大家基本都用Relu。这里不做过多介绍。&lt;/li&gt;
  &lt;li&gt;梯度下降求参数：设代价函数为&lt;script type=&quot;math/tex&quot;&gt;J(\Theta)&lt;/script&gt;, 让代价函数最小的方法是对J函数参数求导，某个参数要更新就是使用下面的公式 &lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;{\partial J(\Theta)} \over {\partial\Theta_j}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;参数解的个数：要么无解，要么1个或者无数个。&lt;/li&gt;
  &lt;li&gt;Huffman编码：类似层次聚类，选择数量最小的2个，两两合并，形成一个二叉树，规定左子树为0，右子树为1。则每个叶子节点的路径用01序列表示，这样做的好处就是编码长度最小，因为数量高的一定靠近根节点处，编码短。&lt;/li&gt;
  &lt;li&gt;特征值和特征向量：所谓特征值是指所有样本在特征向量（单位向量）的投影的方差，特征值越大，信息量越大。这里的特征向量可以理解成X，Y，Z轴这样的正交向量。两个向量的点积A.B就可以理解成A向量在B向量上的投影。&lt;/li&gt;
  &lt;li&gt;矩阵分解：常用的矩阵分解有&lt;strong&gt;PCA，SVD，NMF&lt;/strong&gt;几种，所谓矩阵分解就是要从矩阵中找点隐藏的信息，比如主题什么的。比如&lt;script type=&quot;math/tex&quot;&gt;A_{m*n} = U_{m*k}V_{k*n}&lt;/script&gt; 其中m是样本数，k是主题数，n物品数。矩阵PCA分解的一般步骤：
    &lt;ol&gt;
      &lt;li&gt;减去均值&lt;/li&gt;
      &lt;li&gt;求矩阵的协方差&lt;script type=&quot;math/tex&quot;&gt;\sum_{n*n}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;求协方差的特征值和特征向量，这里的协方差矩阵&lt;script type=&quot;math/tex&quot;&gt;\sum_{n*n}&lt;/script&gt;的TopK特征值就是我们要的K个主题，这里特征向量矩阵即为&lt;script type=&quot;math/tex&quot;&gt;V_{n*k}&lt;/script&gt;。&lt;/li&gt;
      &lt;li&gt;原始矩阵压缩到K维度即为&lt;script type=&quot;math/tex&quot;&gt;A_{m*n}* V_{n*k}&lt;/script&gt;,矩阵存储空间&lt;script type=&quot;math/tex&quot;&gt;m*n 变成m*k+ k*n&lt;/script&gt;. SVD和NMF类同。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;模型基础&quot;&gt;模型基础&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;逻辑回归：逻辑回归当y=1的时候损失为&lt;script type=&quot;math/tex&quot;&gt;-logh_\theta(X)&lt;/script&gt; 也就是&lt;script type=&quot;math/tex&quot;&gt;h_\theta(X)&lt;/script&gt;越接近1，损失越小，当y=0的时候损失则为&lt;script type=&quot;math/tex&quot;&gt;-log(1-h_\theta(X))&lt;/script&gt;。合并在一起，则损失函数为 &lt;script type=&quot;math/tex&quot;&gt;-1/m[\sum_{i=1}^{m}y^{(i)}*logh_\theta(X^{(i)}) + (1-y^{(i)})log(1-h_\theta(X^{(i)}))]&lt;/script&gt; 分类问题通常使用逻辑回归或者softmax解决问题：使用sigmoid函数 &lt;script type=&quot;math/tex&quot;&gt;\sigma(x)=&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{ 1 } \over {1+exp(-h_{w,b}(x))}&lt;/script&gt; 将线性函数强行掰弯，值域在(0,1)之间。如果样本均匀分布则可以设置阈值为0.5.如下图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://p.blog.csdn.net/images/p_blog_csdn_net/chl033/612813/o_SigmoidFunction_701_2.gif&quot; alt=&quot;图示&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;模型求解：使用梯度下降，如果是随机梯度下降SGD：&lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta(y^{(i)} - h_\Theta(x^{(i)}))X_j^{(i)}&lt;/script&gt; ，批量梯度下降MGD则为 &lt;script type=&quot;math/tex&quot;&gt;\Theta_j := \Theta_j - \eta 1 / m\sum_{i=1}^{i=m}(y^{(i)} - h_\Theta(x^{(i)}))X_j^{(i)}&lt;/script&gt;  有意思的最小二乘和逻辑回归使用梯度下降的公式是一样的，这里NG的解释是，嗯，虽然形式一样，但内容不用，里面的y值sigmoid是非线性而在最小二乘中是线性的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;正向传播，反向传播：逻辑回归中其实已经用到了一次反向传播更新参数，当网络有多层的时候，比如MLP神经网络，这里就要链式求导。比如网络比逻辑回归多一层，也就是隐藏层，那么正向传播的公式：&lt;script type=&quot;math/tex&quot;&gt;y=h_\theta(g_w(X))&lt;/script&gt; ，反向传播的公式为：&lt;script type=&quot;math/tex&quot;&gt;W_j:=W_j - \eta *error*h(1-h)\theta*g*(1-g)X_j^{(i)}&lt;/script&gt;,此处可以看到sigmoid的问题就在层数越多，梯度越小，最后梯度就消失了，使用Relu可以避免这个问题，因为Relu的梯度是1，不增也不减。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;词向量&quot;&gt;词向量&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;问题起因：数据挖掘（数据分析，数据清洗，特征处理，模型验证，模型集成）其中特征处理有一项，就是可以给特征做embedding。以前我们做labelencoder及one-hotencoder，虽然可以对category变量进行编码，方便模型进行计算比如NB，SVM，LR，KNN等，前提是特征独立，但大多数情况，特征并不独立，比如“麦克”和“话筒”。如果one-hot化之后，他们之间的相似度是0.&lt;/li&gt;
  &lt;li&gt;解决思路：词周围的词决定了当前词的意义，那么我们可否用周围词出现的数量来标记当前词呢，当然是可以的。Globalvec就是这么做的。但数量的刻画毕竟太过简单，是否有一个更好的拟合函数来刻画&lt;script type=&quot;math/tex&quot;&gt;P(W_i \|Context_{W_i})&lt;/script&gt;呢？当然也是可以的，这里就用到了一层感知机模型，用无指导的方式，训练，最大似然该拟合函数。实际上就是多个LR模型的训练，参考 &lt;img src=&quot;http://img.blog.csdn.net/20140525173342578&quot; alt=&quot;CBOW+HS&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;源码分析：目前我们只分析CBOW+HS模式，其他模式比如SkipGram+NS是类似的。
    &lt;ol&gt;
      &lt;li&gt;初始化：比如设置向量维度是50维度，输入Syn0向量保存了词汇库所有不重复词V的向量，初始化不为零的小数。Syn1存放huffman树的非叶节点参数，也是50维度，初始设置为0。窗口可以设置为5。&lt;/li&gt;
      &lt;li&gt;训练：&lt;img src=&quot;http://img.blog.csdn.net/20140525182637750&quot; alt=&quot;源码1&quot; /&gt; &lt;img src=&quot;http://img.blog.csdn.net/20140525182645906&quot; alt=&quot;源码2&quot; /&gt; 当一句读进来，每一个词W都会有上下文向量（不包括自己），将50维向量累加，形成向量nelu1,对于W的huffman树有个编码，比如1100，那么将执行4词逻辑回归，每次逻辑回归，都会按照公式更新参数，Syn1参数更新公式&lt;script type=&quot;math/tex&quot;&gt;Syn1[i]:=Syn1[i] - \eta(y - nelu1*syn1)*nelu1&lt;/script&gt; 这里y是树编码的某个位的数值，累计残差为 &lt;script type=&quot;math/tex&quot;&gt;$nelue += \eta(y - nelu1*syn1)* syn1&lt;/script&gt; 然后更新Syn0，注意不是更新nelu1,而是更新Syn0 &lt;script type=&quot;math/tex&quot;&gt;Syn0 += nelue&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;结束：一轮迭代2G文本，大概10亿字，训练2-3小时即可。由于word2vec并不是神经网络，不含有隐层，所以做了N次逻辑回归，反向传播就一次，速度是非常快的。训练完毕，每个词向量就有了，使用PCA进行降维，可以把一些有意思的词可视化出来。对于向量的可视化，可以参考multidimensional scaling，也挺有意思的。&lt;/li&gt;
      &lt;li&gt;有关算法提速：通过hs加速训练，NS也可以。另外exp指数计算采用的查表方式，NS和HS其实可以混合使用。另外&lt;strong&gt;多线程并没有加锁&lt;/strong&gt;，这个问过作者，说多次梯度下降没有问题，就算重复更新也无妨。有兴趣的可以参考&lt;a href=&quot;http://blog.csdn.net/mytestmy/article/details/26961315&quot;&gt;博文&lt;/a&gt;，这位博主写的挺好。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;学着使用mathjax，公式写的有点仓促，但挺好用的哈，参考&lt;a href=&quot;http://m.blog.csdn.net/article/details?id=46757757&quot;&gt;http://m.blog.csdn.net/article/details?id=46757757&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;word2vec写的不错的博客地址&lt;a href=&quot;http://blog.csdn.net/mytestmy/article/details/26969149&quot;&gt;http://blog.csdn.net/mytestmy/article/details/26969149&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PCA、SVD的内容参考&lt;a href=&quot;http://blog.sina.com.cn/s/blog_66a6172c0102v1v7.html&quot;&gt;http://blog.sina.com.cn/s/blog_66a6172c0102v1v7.html&lt;/a&gt;,写的比较通俗&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 30 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/05/30/from-word2vec.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/05/30/from-word2vec.html</guid>
        
        <category>machine learning</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>利用Jekyll在Github搭建博客</title>
        <description>&lt;p&gt;这周末刚好项目不那么忙，抽空利用jekyll在本地上搭建博客，然后推送到自己的&lt;a href=&quot;https://bobz653.github.io&quot; title=&quot;博客&quot;&gt;Github博客&lt;/a&gt;中，对日常中好奇，好玩，复习的知识进行记录，也分享给感兴趣的朋友。&lt;/p&gt;

&lt;h2 id=&quot;1环境准备&quot;&gt;1.环境准备&lt;/h2&gt;
&lt;p&gt;我是在mac中搭建的，所以其他环境请找相应的安装包。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.安装Jekyll：使用&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
gem -v;
gem update --system;
sudo gem install jekyll -n /usr/local/Cellar/;
jekyll -v;
&lt;/pre&gt;
&lt;p&gt;如果可以正常显示版本信息，说明安装成功。&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;2.github 博客：在&lt;a href=&quot;https://github.com&quot;&gt;Github&lt;/a&gt;注册账户&lt;/p&gt;

  &lt;p&gt;新建一个以用户名.github.io为名称的工程即可，这个工程有且仅有一个，是github使用gitpages服务托管的仅支持jekyll这样的静态页面的服务。在本地使用ssh-genkey创建rsa_pub文件，加入到git的ssh pubkey中即可通过&lt;/p&gt;
  &lt;pre&gt;git clone;
git push origin master
&lt;/pre&gt;
  &lt;p&gt;等命令在本地博客更新后，来更新线上博客了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2搭建博客&quot;&gt;2.搭建博客&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;1.合适的模板：在jeklly中有很多免费的模板供大家使用，比如&lt;a href=&quot;https://jekyllthemes.io/&quot;&gt;Jekyll theme&lt;/a&gt;，找到一个就git clone下来吧。&lt;/p&gt;

  &lt;p&gt;2.本地运行：&lt;/p&gt;
  &lt;pre&gt;
jekyll build;
jekyll server;
&lt;/pre&gt;
  &lt;p&gt;使用http://127.0.0.1:4000/即可在本地查看。&lt;/p&gt;

  &lt;p&gt;3.修改：&lt;/p&gt;

  &lt;p&gt;文章在_post文件夹中，全局配置在_config.yml中，css在_sass中,布局在_layout中。修改后所有的网站最终文件在_site文件夹中。&lt;/p&gt;

  &lt;p&gt;新建的文章都在_post文件夹中，以YYYY-MM-DD-title.md方式存在。且头部是一些模板需要读取的信息，比如作者，类别，标题，标签等信息。可以用rake命令自动生成，定义好Rakefile之后，使用&lt;/p&gt;
  &lt;pre&gt;rake post title='hello world' &lt;/pre&gt;
  &lt;p&gt;这样的命令即可。
4.针对公式编辑在概率，算法中常常用到，我们可以使用mathjax来进行。&lt;/p&gt;

  &lt;pre&gt; &lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;
formula1: $$n==x$$
formula2: $$n!=x$$
&lt;/pre&gt;
  &lt;p&gt;效果：
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt; &lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;formula1: &lt;script type=&quot;math/tex&quot;&gt;n==x&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;formula2: &lt;script type=&quot;math/tex&quot;&gt;n!=x&lt;/script&gt;&lt;/p&gt;

  &lt;p&gt;5.当然也可以加一些分享，评论的JS到博客中，在_layout中可以修改post模板，以增加这些功能。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3推送到线上&quot;&gt;3.推送到线上&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;有前面的准备，这个步骤就简单了啊，直接使用&lt;/p&gt;
  &lt;pre&gt;
git add -A ;#这里是加入删除选项
git commit -m 'comments';
git push origin master;
&lt;/pre&gt;

  &lt;p&gt;然后访问https://youname.github.io 即可，可能有几秒的延迟。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 05 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/%E6%9D%82%E8%B0%88/2017/05/05/hello-world.html</link>
        <guid isPermaLink="true">http://localhost:4000/%E6%9D%82%E8%B0%88/2017/05/05/hello-world.html</guid>
        
        <category>jekyll github</category>
        
        
        <category>杂谈</category>
        
      </item>
    
  </channel>
</rss>
