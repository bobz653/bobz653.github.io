---
layout: post
title: "从最小二乘（OLS）和最大似然估计（MLE）的关系谈起"
description: "OLS MLE的关系"
author: "张波"
category: 机器学习
tags: [machine learning]
---

最近一段时间一直在忙碌工程项目上的事情，虽琐碎但也比较充实。了解到了大公司产研测的流转。使用devops类似的工具（具体内部工具就不说了）来去管理需求、开发、测试、上线部署、监控流程，实现进度和代码review等质量一系列的监管。开源的大家可以参考 **Gitlab + Phabircator**. 开发规范可以参考**Git-flow**。这不是本篇文章的重点，以后有时间可以详细说，还是回到算法模型上吧。

## 机器学习三要素
* 模型：这里我们可以定义为非条件概率模型和条件概率模型，说的通俗一些就是常说的在从输入空间到输出空间的转化方式。

    1.	非条件概率： $$F=\{f\mid Y=f_\theta(X)\}$$ 对应我们的判别式模型，比如LR SVM RNN CRF MEMM等，由于这种模型的偏差较小，方差较大，适用于数据量大的情况
    2.	条件概率：$$F = \{ P\mid P_\theta(Y\mid X),\theta \in R^n\}$$ 对应我们的产生式模型，比如 NB HMM等，这种模型偏差大，方差小适用于数据量较小的情况。
    3.	怎么选择模型：有人问怎么调和这些方差偏差，有一种套路，就是偏差大，欠拟合那就模型复杂化，多来点特征，多上几层。方差大那就是过拟合，那就多来点数据，来点正则，去掉一些不重要的特征，神经网络的话就加上dropout。

* 目标：定义什么样的损失函数来作为优化的目标,常用
	1. 最小二乘OLS也称为平方损失：$$ L(Y,f(x)) = (Y-f(x))^2 $$，常见模型线性回归
	2. 最大似然（取上对数就是对数似然）：$$ L(P,P(Y\mid X)) = -log(P(Y\mid X))$$ 常见模型比如LR，Softmax
	3. Hinge-Loss: $$ max(0,1-Y*f_\theta(X))$$ 常见模型比如SVM
	4. 指数: $$exp(Y*f_\theta(X))$$,常见的模型比如Adaboost，GBDT

* 算法
   1. 最小二乘法
   2. BGD,SDG,MBGD
   3. 


