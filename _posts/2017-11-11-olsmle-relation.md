---
layout: post
title: "从似然MLE推导最小二乘OLS谈机器学习三要素"
description: "OLS MLE的关系"
author: "张波"
category: 机器学习
tags: [machine learning]
---

最近被人问到最大似然，最小二乘，梯度下降，我发现一不留神很容易搞不清楚概念：对于机器学习三要素的问题，定义目标和怎么用算法达到这个目标的事儿，这几个概念清楚了，就知道梯度下降和前两者是不同的。下午写篇博客来梳理记录一下：

> 博客很久没有更新了，这段时间一直在琢磨工程项目上的事情，虽琐碎但比较充实。团队有关产研测的规范流转，使用devops类似的工具（具体内部工具就不说了）来去管理需求、开发、测试、上线部署、监控流程，实现进度和代码review等质量一系列的监管。开源的大家可以参考 **Gitlab + Phabircator**. 开发规范可以参考**Git-flow**。这不是本篇文章的重点，以后有时间可以详细说，
> 


## 机器学习三要素
* 模型：这里我们可以定义为非条件概率模型和条件概率模型，说的通俗一些就是常说的在从输入空间到输出空间的转化方式。

    1.	**非条件概率**： $$F=\{f\mid Y=f_\theta(X)\}$$ 对应我们的判别式模型，比如LR SVM RNN CRF MEMM等，由于这种模型的偏差较小，方差较大，适用于数据量大的情况
    2.	**条件概率**：$$F = \{ P\mid P_\theta(Y\mid X),\theta \in R^n\}$$ 对应我们的产生式模型，比如 NB HMM等，这种模型偏差大，方差小适用于数据量较小的情况。
    3.	**怎么选择模型**：有人问怎么调和这些方差偏差，有一种套路，就是偏差大，欠拟合那就模型复杂化，多来点特征，多上几层。方差大那就是过拟合，那就多来点数据，来点正则，去掉一些不重要的特征，神经网络的话就加上dropout。

* 目标：定义什么样的损失函数来作为优化的目标,常用
 
	1. **最小二乘OLS也称为平方损失**：$$ L(Y,f(x)) = (Y-f(x))^2 $$，常见模型比如线性回归
	2. **最大似然（取上对数就是对数似然）**：$$ L(P,P(Y\mid X)) = -log(P(Y\mid X))$$ 常见模型比如LR，Softmax
	3. **Hinge-Loss**: $$ max(0,1-Y*f_\theta(X))$$ 常见模型比如SVM
	4. **指数**: $$exp(Y*f_\theta(X))$$,常见的模型比如Adaboost，GBDT
	5. **0，1损失**：比如分类模型LR

* 算法
   1. **最小二乘估计**：对不同的参数求导=0，用方程直接求出参数，这种方式适合数据量比较小的情况。
   2. **BGD,SDG,MBGD**：适合数据量比较大的情况，使用梯度下降方式去逐渐逼近目标的最小值或者最大值。

## 条件概率（极大似然）推导到非条件（最小二乘）
* 结论

  1. 两者是等价。也就是两者目标在服从正态分布的情况下是可以发现目标是一致的
  2. 既然目标是一致的，那么算法也可以通过，也就是可用最小二乘估计来计算LR的似然目标，但一般不这么做。OLS不善于做0/1回归，目标值太小，毕竟不是普通的连续Y值的回归。

* 推理
  1. 最大似然：假设Y值服从正态分布 $$Y_i \approx N(f(x_i),\sigma^2) $$ 且是独立分布,写成似然函数，就是 $$ P(Y_i\mid X_i) \approx exp(Y_i - f(X_i))^2  $$ 我这里省略了正态分布的其他部分，公式在电脑上是在敲的费劲啊~
  2. 由于概率独立分布，把所有Yi的概率相乘，取对数后求最大值，就会变成$$ min(\Sigma (Y_i - f(X_i))^2) $$
  3. 怎么样，现在看着就是最小二乘的公式了吧
  
## LR的似然函数和最小二乘的关系

* 怎么LR的推导不出来？不是正态分布嘛，当然推导不出来。
  1. LR的似然 $$P(Y_i\mid X_i) = h_\theta (x_i)^{Y_i} *  (1-h_\theta (x_i))^{(1-Y_i)} $$ 同样可以取LOG后相加，就是所有概率的相乘啦。
  
  2. LR的似然为啥和最小二乘那么不同呢，好像推导不出来，没有关系，我先声明了，Y要服从正态分布，可是LR的Y的0 1分布，很显然是bernoulli分布 $$y \approx Bernoulli(\phi)$$
  
  3. NG说，你看他们求导后，梯度怎么看着是一模一样的，哈哈，确实，但 $$\theta_i -=（f(x^{(j)})-y^{(j)})x_i^{(j)} $$ 中的f函数是不同的哈，LR中的是有sigmoid非线性变换的。

> 后记：对于算法，我自己写了一些模型的基础实现，方便更清楚给同学解释 LR，SVM，C4.5，NN，NB是怎么回事哈，待我整理好了再发，写一篇文章不留神3个小时过去了~~~啊啊啊

## 参考
1. [最小二乘、极大似然、梯度下降有何区别？](https://www.zhihu.com/question/24900876/answer/65176508)
2. [最大似然估计和最小二乘估计的区别与联系](http://blog.csdn.net/xidianzhimeng/article/details/20847289)
  


